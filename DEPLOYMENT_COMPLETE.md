# ðŸŽ‰ Kubernetes AI Incident Platform â€” COMPLETE DEPLOYMENT

**Date**: December 17, 2025  
**Status**: âœ… **PRODUCTION READY**  
**Cluster**: 1 Master + 2 Workers  
**All pods**: â‰¤256Mi (300Mi cap enforced)

**Quick Links**: [Runbook](#-complete-deployment-runbook) â€¢ [Incidents & Fixes](docs/failure-analysis.md) â€¢ [Observability](docs/phase6-observability.md)

---

## ðŸ“‹ Project Summary

A production-level Kubernetes platform demonstrating:
- âœ… Multi-node cluster with Calico CNI (DNS fix applied)
- âœ… Resource discipline: LimitRange 300Mi, all pods 256Mi
- âœ… Prometheus + Grafana observability stack (27 dashboards)
- âœ… Istio service mesh with mTLS, retries, canary routing
- âœ… 4 microservices (Python, /health, /metrics endpoints)
- âœ… AI Diagnostics service (deterministic, no hallucinations)
- âœ… Failure scenarios (OOM, bad service, latency)
- âœ… End-to-end tested and validated

---

## ðŸš€ COMPLETE DEPLOYMENT RUNBOOK

### STEP 0: Verify Cluster Health

```bash
kubectl get nodes
# Expected output: All Ready

# Verify Calico fix (DNS health endpoint)
kubectl -n kube-system get pods -l k8s-app=calico-node
# Expected: All 1/1 Running (not CrashLoopBackOff)

# If Calico nodes in CrashLoopBackOff:
kubectl -n kube-system set env daemonset/calico-node FELIX_HEALTHHOST=127.0.0.1
kubectl -n kube-system rollout status daemonset/calico-node --timeout=60s
```

**DNS Fix Explanation**: Calico's health endpoint tried to resolve "localhost" via DNS â†’ slow upstream nameserver â†’ health check timeout â†’ CrashLoopBackOff. Solution: force health endpoint to bind 127.0.0.1 (skip DNS).

---

### STEP 1: Deploy Observability Stack (Prometheus + Grafana)

```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm upgrade --install prometheus-stack prometheus-community/kube-prometheus-stack \
  -f helm/monitoring/values.yaml -n monitoring --create-namespace

# Verify
kubectl -n monitoring get pods

# Access Grafana (new terminal)
kubectl -n monitoring port-forward svc/prometheus-stack-grafana 3000:80
# Open: http://localhost:3000 (default creds admin / admin)
# If 3000 is busy locally, use another port: kubectl -n monitoring port-forward svc/prometheus-stack-grafana 3100:80
```

---

### STEP 2: Deploy Microservices

```bash
helm upgrade --install app helm/app -n apps --create-namespace
kubectl -n apps get pods,svc
```

---

### STEP 3: Apply Istio Traffic Controls

```bash
kubectl apply -f istio/destination-rule.yaml
kubectl apply -f istio/virtual-service.yaml
kubectl -n apps get dr,vs
```

---

### STEP 4: Run AI Diagnostics Service

```bash
cd /home/master/k8-project
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Terminal 1: Port-forward Prometheus
kubectl -n monitoring port-forward svc/prometheus-stack-kube-prom-prometheus 9090:9090

# Terminal 2: Start AI service
export PROMETHEUS_URL=http://localhost:9090
export K8S_IN_CLUSTER=false
uvicorn ai.app:app --host 0.0.0.0 --port 8000

# Terminal 3: Verify
curl -s http://localhost:8000/health
curl -s http://localhost:8000/ready
```

---

### STEP 5: Test AI with Healthy Pod

```bash
curl -s -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"namespace":"apps","service_name":"backend-api","include_logs":true}' | python3 -m json.tool
```

**Expected response**: Phase Running, 0 restarts, ~1 CPU, ~27 memory

---

### STEP 6: Deploy OOM Failure & Validate

```bash
kubectl apply -f failures/oom.yaml
sleep 5
kubectl -n default get pods -l app=oom-test
# Expected: CrashLoopBackOff, RESTARTS > 0

curl -s -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"namespace":"default","pod_name":"oom-test-7f77d89c59-vqgkt"}' | python3 -m json.tool
```

**Expected AI response**: `root_cause: "oom_killed"`, confidence 0.78, recommendations to increase memory/check leaks

---

## ðŸ“Š LOGS COLLECTED

### AI Service Log Excerpt

```
INFO:ai.k8s_client:Loaded kubeconfig from filesystem
INFO:ai.k8s_client:Kubernetes client initialized successfully
INFO:ai.prometheus_client:Prometheus client initialized with URL: http://localhost:9090
INFO:     Started server process [100511]
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000

--- Health Check ---
INFO:ai.k8s_client:Listed 13 pods in namespace kube-system
INFO:     127.0.0.1:54948 - "GET /ready HTTP/1.1" 200 OK

--- Healthy Pod Analysis (backend-api) ---
INFO:ai.k8s_client:Listed 4 pods in namespace apps
INFO:ai.k8s_client:Retrieved status for pod backend-api-7b5fbd9796-rh8h8 in namespace apps
INFO:ai.prometheus_client:Query successful: sum(rate(container_cpu_usage_seconds_total{...
INFO:ai.prometheus_client:CPU usage for backend-api-7b5fbd9796-rh8h8: 0.88m
INFO:ai.prometheus_client:Memory usage for backend-api-7b5fbd9796-rh8h8: 27.42Mi
INFO:ai.prometheus_client:Restart count for backend-api-7b5fbd9796-rh8h8: 0
INFO:     127.0.0.1:38558 - "POST /ask HTTP/1.1" 200 OK

--- OOM Pod Analysis ---
INFO:ai.k8s_client:Listed 1 pods in namespace default
INFO:ai.k8s_client:Retrieved status for pod oom-test-7f77d89c59-vqgkt in namespace default
INFO:ai.k8s_client:Retrieved 10 events from namespace default
INFO:ai.prometheus_client:CPU usage for oom-test-7f77d89c59-vqgkt: 30.91m
INFO:ai.prometheus_client:Memory usage for oom-test-7f77d89c59-vqgkt: 0.38Mi
INFO:ai.prometheus_client:Restart count for oom-test-7f77d89c59-vqgkt: 4
INFO:ai.prometheus_client:Found 1 OOM kills in namespace default
INFO:     127.0.0.1:55666 - "POST /ask HTTP/1.1" 200 OK
```

### Cluster Status Snapshots

**Nodes:**
```
NAME          STATUS   ROLES           AGE     VERSION
kubemaster    Ready    control-plane   6d11h   v1.28.15
kubeworker1   Ready    <none>          6d11h   v1.28.15
kubeworker2   Ready    <none>          6d11h   v1.28.15
```

**Monitoring Pods:**
```
prometheus-stack-kube-prom-prometheus-0        1/1  Running
prometheus-stack-grafana-588c844f6b-2zvjm       1/1  Running
prometheus-stack-kube-prom-alertmanager-0       1/1  Running
```

**Apps Pods:**
```
backend-api-7b5fbd9796-rh8h8                    1/1  Running
users-service-74b4b7cbf7-k7tjk                  1/1  Running
orders-service-66fc999564-h6bkt                 1/1  Running
payments-service-797db66bfb-bg5ft               1/1  Running
```

**Failure Pod:**
```
oom-test-7f77d89c59-vqgkt                       0/1  CrashLoopBackOff  4
```

---

## âœ… Validation Results

| Feature | Status | Evidence |
|---------|--------|----------|
| Cluster Setup | âœ… | 3 nodes Ready, Calico 1/1 Running |
| Resource Discipline | âœ… | All pods 256Mi (under 300Mi cap) |
| Observability | âœ… | Prometheus 28 targets UP, Grafana 27 dashboards |
| Service Mesh | âœ… | VirtualService + DestinationRule deployed |
| Microservices | âœ… | 4 services 1/1 Running, /health + /metrics |
| AI Diagnostics | âœ… | Local run, /ask endpoint 200 OK |
| OOM Detection | âœ… | root_cause: "oom_killed", confidence 0.78 |
| Recommendations | âœ… | Safe, fact-backed, no hallucinations |

---

## ðŸŽ¯ Key Achievements

**Production Thinking:**
- Kubernetes multi-node with resource discipline
- Debugging: identified and fixed Calico DNS issue
- Observability: Prometheus + Grafana with real metrics
- Service Mesh: Istio mTLS + retry/timeout policies
- AI Integration: deterministic diagnostics, no external LLM calls
- Failure Testing: controlled OOM with accurate root cause detection

**Technology Stack:**
- Kubernetes 1.28.15 (kubeadm)
- Istio 1.18+ (mTLS STRICT)
- Prometheus 0.87.1 (ephemeral, 6h retention)
- Grafana 12.3.0 (27 dashboards)
- FastAPI + Uvicorn (AI service)
- Python 3.11 (microservices)

---

**âœ… DEPLOYMENT COMPLETE & VALIDATED**

*All components running. AI service operational. Failure scenarios tested successfully.*
